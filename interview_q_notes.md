Questions:

First questions:

Lab 4:

Why did I choose my neural network block architecture? I said it's the standard and symmetry is nice. 

Then asked could I use a different activation function to Relu, and remove one of my regularization layers (i.e. 
my batch norm or dropout)? I wasn't too sure (I don't know if he was either tbh), but I said that RELU has regularization 
built in (anything negative input goes to 0).


Final question:

What's the effect of batch size on the training of the network. 

I maxed out the batch size so it fit on ram (as a power of 2)

I said with a larger batch size, it would be faster, and that the gradients would be smoother as its more 
representative of the whole dataset. 

