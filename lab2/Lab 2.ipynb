{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1664296311485,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "lCegEqO2QEuX"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This notebook is for the exploration of Logistic Regression -- it corresponds to handout-02\n",
    "# https://frcs.github.io/4C16-LectureNotes/logistic-regression.html\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20347,
     "status": "ok",
     "timestamp": 1664296331829,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "6jz8F8K2QIKl",
    "outputId": "2297c46e-930a-445c-c911-da89bac6f9b2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/MyDrive/4c16-labs/code/lab-02/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1664297216275,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "K0UGq_mpQEua",
    "outputId": "6e9ba0a9-6573-460f-fc19-9a9c3acff286"
   },
   "outputs": [],
   "source": [
    "# Autoload setup (you don't need to edit this cell); instructions to: \n",
    "#   i) enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "#  ii) import the module 'lab_1' (which will contain your functions) in an autoreloadable way \n",
    "%aimport lab_2\n",
    "# iii) indicate that we want autoreloading to happen on every evaluation.\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1479,
     "status": "ok",
     "timestamp": 1664297220729,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "RCskJGRiQEua"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 1: import\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "import pandas as pd        # for datasets\n",
    "import numpy as np         # for linear algebra\n",
    "from tqdm import tqdm      # for progress bars \n",
    "\n",
    "import matplotlib as mpl   # for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1664297221708,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "s8tayQyZoa-J"
   },
   "outputs": [],
   "source": [
    "# sets default style for graphs\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['lines.linewidth'] = 1.7\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1664297237358,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "1PjSYTPZQEub",
    "outputId": "c7b85407-a0f7-4657-ac89-7b84a9ca684d"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 2. loading the ISLR 'Default' dataset\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "# see https://cran.r-project.org/web/packages/ISLR/ISLR.pdf\n",
    "#\n",
    "# This data set contains information on ten thousand customers. \n",
    "#\n",
    "# The aim here is to predict which customers will default on their credit card debt.\n",
    "#\n",
    "# The dataset contains 10000 observations on the following 4 variables.\n",
    "#   * 'default': a No/Yes label indicating whether the customer defaulted on their debt\n",
    "#   * 'student': a No/Yes label indicating whether the customer is a student\n",
    "#   * 'balance': the average balance that the customer has remaining on their credit card after making\n",
    "#                their monthly payment\n",
    "#   * 'income' : income of customer\n",
    "\n",
    "df = pd.read_csv('Default.csv')\n",
    "\n",
    "# we are using here the pandas python package to read the CSV file.\n",
    "# we can look at the first 10 observations\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1664297283869,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "QPzxIfwnQEub",
    "outputId": "721f798a-c29b-4db3-b8c8-513bcdf380fa"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 3. visualise your data\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "# We need to visualise our data\n",
    "# let's ignore the categorical features 'student' and the first column\n",
    "# and focus on the 'balance' and 'income' features\n",
    "\n",
    "balance = df['balance'].values\n",
    "income = df['income'].values\n",
    "\n",
    "# the outcome is of boolean type, converting it to vector of integers\n",
    "y = (df['default'].values == 'Yes').astype(int)\n",
    "\n",
    "# we are only going to plot a subset of the data\n",
    "income_subset = income[0:1000];\n",
    "balance_subset = balance[0:1000];\n",
    "y_subset = y[0:1000];\n",
    "\n",
    "# plotting balance vs income for the 'No Default' class\n",
    "plt.scatter(balance_subset[y_subset == 0],\n",
    "            income_subset[y_subset == 0], \n",
    "            s=15, marker='o')\n",
    "\n",
    "# plotting balance vs income for the 'Default' class\n",
    "plt.scatter(balance_subset[y_subset == 1],  \n",
    "            income_subset[y_subset == 1],  \n",
    "            s=40, marker='+')\n",
    "\n",
    "plt.ylim(ymin=0)\n",
    "plt.ylabel('Income')\n",
    "plt.xlim(xmin=-100)\n",
    "plt.xlabel('Balance')\n",
    "plt.legend(['No Default', 'Default'])\n",
    "plt.show()\n",
    "\n",
    "# after a quick look at the graph, it appears that \n",
    "# the most relevant feature is 'balance'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1664297343107,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "Y5p7L8_hQEuc",
    "outputId": "b05784d0-e91f-4f05-8396-06c1a799a0ca"
   },
   "outputs": [],
   "source": [
    "# Exercise 1: implement cross-entropy (in the lab_2 module)\n",
    "\n",
    "w_test = np.array([[0.1], [0.3]])\n",
    "X_test = np.array([[1,1], [1,0], [3,2]])\n",
    "y_test = np.array([[0], [1], [1]])\n",
    "print(lab_2.cross_entropy(w_test, X_test, y_test))\n",
    "\n",
    "# Should print 0.632853327993248 if your function is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1664297411689,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "tcDWL13zQEuc",
    "outputId": "e884b9b0-0b15-4898-fd9b-f184c7a73270"
   },
   "outputs": [],
   "source": [
    "# Exercise 2: implement gradient computation (in the lab_2 module)\n",
    "\n",
    "w_test = np.array([0.1, 0.3])\n",
    "X_test = np.array([[1,1], [1,0]])\n",
    "y_test = np.array([0, 1])\n",
    "print(lab_2.gradient(w_test, X_test, y_test))\n",
    "# Expected result: [ 0.06183342  0.29934383]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1664297423151,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "oUpUeO08QEud"
   },
   "outputs": [],
   "source": [
    "# The gradient descent algorithm as explained in lecture notes\n",
    "# the learning_rate refers to the greek letter 'eta'\n",
    "# The method also returns the vectors of \n",
    "#   loss function, sampled iterations, weights\n",
    "\n",
    "def gradient_descent(w0, X, y, learning_rate=0.01, maxit=1000):\n",
    "    print(\"starting gradient descent\")\n",
    "    w = w0\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    step = 83 # interval between measures\n",
    "    m = (maxit // step) + 1 # nb of checks\n",
    "    loss = np.zeros((m,1))\n",
    "    its = np.zeros((m,1))\n",
    "    ws = np.zeros((m,p))\n",
    "\n",
    "    for i in tqdm(range(0, maxit)):\n",
    "        # monitoring the loss at regular intervals      \n",
    "        if (i % step == 0):\n",
    "            j = i // step\n",
    "            loss[j] = lab_2.cross_entropy(w, X, y)\n",
    "            its[j] = i\n",
    "            ws[j,0:p] = w.ravel()\n",
    "            \n",
    "        # computing the loss gradient\n",
    "        grad = lab_2.gradient(w,X,y);\n",
    " \n",
    "        # updating the weights\n",
    "        w = w - learning_rate * grad;        \n",
    "        i = i + 1\n",
    "    return w, loss, its, ws\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1664297436825,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "Y6AyRNO5Z1Ly",
    "outputId": "0b6fc82c-a593-44b8-9fbf-80cf12e93648"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Setting up the Model and Design Matrix\n",
    "#\n",
    "# in this lab we will look at the following model:\n",
    "# y = [ w0 + w1 * balance > 0]\n",
    "#\n",
    "# and ignore the 'income' feature\n",
    "\n",
    "n = balance.shape[0]  # number of observations\n",
    "p = 2                 # number of features\n",
    "\n",
    "X = np.zeros(shape=(n, p))\n",
    "print(X.shape)\n",
    "\n",
    "# the first feature is 1 (with associated weight w0)\n",
    "X[:,0] = 1;\n",
    "\n",
    "# the second feature is the balance values (with associated weight w1)\n",
    "X[:,1] = balance[:]\n",
    "print(X.shape)\n",
    "\n",
    "# initial weights\n",
    "w0 = np.array([0,0]);\n",
    "\n",
    "# starting gradient descent optimisation with some default parameters\n",
    "w, loss, its, ws = gradient_descent(w0, X, y, learning_rate=40, maxit=5000);\n",
    "print(w)\n",
    "\n",
    "# plot of the loss graph, ie.\n",
    "plt.plot(its, loss)\n",
    "plt.title(\"Loss Graph\")\n",
    "plt.xlabel('gradient descent iterations')\n",
    "plt.ylabel('average cross-entropy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3lIPqqApsk0"
   },
   "outputs": [],
   "source": [
    "# if you look carefully, you will find that the gradient descent is not working \n",
    "# very well. You can see this because the overall loss is not going down, oscillates \n",
    "# and never gets low enough (eg. avg cross-entropy > .5, when we would like something < 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1664297455310,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "h6KGloEtGpty"
   },
   "outputs": [],
   "source": [
    "# to better study the convergence issues, we'll also plot the weights \n",
    "# we encapsulate all these plots a single function:\n",
    "\n",
    "def plot_loss_graph_and_weight_evolution(loss, ws, its):\n",
    "\n",
    "  f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "  f.set_figheight(15)\n",
    "  f.set_figwidth(15)\n",
    "\n",
    "  ax1.plot(its, loss)\n",
    "  ax1.set_xlabel('gradient descent iterations')\n",
    "  ax1.set_ylabel('average cross-entropy')\n",
    "\n",
    "  ax2.plot(ws[:,0], ws[:,1], \n",
    "          color='g', marker='o', \n",
    "          linestyle='dashed', linewidth=0.5, markersize=4)\n",
    "  ax2.set_title('Evolution of the Weights')\n",
    "  ax2.set_xlabel('$w_0$')\n",
    "  ax2.set_ylabel('$w_1$')\n",
    "\n",
    "  ax3.plot(its, ws[:,0], 'g-')\n",
    "  ax3.set_title('Evolution of $w_0$')\n",
    "  ax3.set_xlabel('gradient descent iterations')\n",
    "  ax3.set_ylabel('$w_0$')\n",
    "\n",
    "  ax4.plot(its, ws[:,1], 'g-')\n",
    "  ax4.set_title('Evolution of $w_1$')\n",
    "  ax4.set_xlabel('gradient descent iterations')\n",
    "  ax4.set_ylabel('$w_1$')\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 2439,
     "status": "ok",
     "timestamp": 1664297461774,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "6qesA1hRHJ3t",
    "outputId": "1376fd8c-43cf-409f-d62c-ec5899bf146b"
   },
   "outputs": [],
   "source": [
    "# plotting the evloution of loss + weights\n",
    "\n",
    "plot_loss_graph_and_weight_evolution(loss, ws, its)\n",
    "\n",
    "# we can see that the weights are diverging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 772
    },
    "executionInfo": {
     "elapsed": 4840,
     "status": "ok",
     "timestamp": 1664297486597,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "yaYh32oniFhY",
    "outputId": "16bc2af5-1b46-4c32-b617-9ee17bafeb7e"
   },
   "outputs": [],
   "source": [
    "# One problem here is that Gradient Descent algorithms are quite sensitive \n",
    "# to initial conditions and it is often beneficial to rescale or offset \n",
    "# the features to improve the performance. \n",
    "#\n",
    "# In our case we note that balance ranges from 0 to about 2500.\n",
    "# this is much larger in magnitude to the first feature which is simply 1.\n",
    "# Thus we (arbitrarily) rescale by factor of 1/1000\n",
    "\n",
    "X[:,1] = balance[:]/1000;\n",
    "\n",
    "# note to self: we'll need to apply this rescaling  \n",
    "# everytime we are making predictions.\n",
    "\n",
    "# starting again Gradient Descent with same parameters\n",
    "w, loss, its, ws = gradient_descent(w0, X, y, learning_rate=40, maxit=5000);\n",
    "print(w)\n",
    "\n",
    "# plot of the loss graph\n",
    "plot_loss_graph_and_weight_evolution(loss, ws, its)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUJthsUh4KMf"
   },
   "outputs": [],
   "source": [
    "# we can see that we are doing much better, with the loss going down to about an\n",
    "# avg cross entropy of 0.1, but it still seems to be oscillating,\n",
    "# which indicates that the learning rate is probably too high (see lecture notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1664285066525,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "KrH7OTWVQEue",
    "outputId": "e940e841-f959-4c83-dea2-8e2122029b32"
   },
   "outputs": [],
   "source": [
    "# note that you can also check if you reached convergence by checking if the \n",
    "# gradient is null (or at least very small):\n",
    "\n",
    "print(lab_2.gradient(w, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 772
    },
    "executionInfo": {
     "elapsed": 4572,
     "status": "ok",
     "timestamp": 1664297543215,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "8YQxoSWEQEue",
    "outputId": "a500618a-e094-498d-88c2-b7a66194f5d2"
   },
   "outputs": [],
   "source": [
    "# Exercise: study the convergence for a few different learning rates, \n",
    "# and observe the different behaviours.\n",
    "# for instance you can try lr values in {150, 30, 10, 1, 0.001}\n",
    "\n",
    "lr = 150 # change learning rate here\n",
    "w0 = np.array([0,0]);\n",
    "print(f\"learning rate = {lr}\")\n",
    "w, loss, its, ws = gradient_descent(w0, X, y, learning_rate=lr, maxit=3000);\n",
    "plot_loss_graph_and_weight_evolution(loss, ws, its)\n",
    "\n",
    "# Question 3 \n",
    "# Find the learning rate that allows us to converge the fastest to the global optimum.\n",
    "# eg. lr = 45 (doesn't have to be super precise, push to assessment to check if answer is ok)\n",
    "# write answer in question_3 of lab_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2984,
     "status": "ok",
     "timestamp": 1664297557738,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "tlf_xI_OQEue",
    "outputId": "d822d3cd-36c4-4a98-a0ae-8804eb2b93a1"
   },
   "outputs": [],
   "source": [
    "# In logistic Regression, we set a parametric model for the likelihood \n",
    "# we denote logit = x'w and parametrise the likelihood as\n",
    "# p(y_i=1|logit) = 1/(1 + exp(-logit))\n",
    "#\n",
    "# We want to verify that this is a correct approximation for our problem\n",
    "#\n",
    "# The following function makes an empirical measurement of p(y_i=1|logit)\n",
    "# by recording in the dataset the proportion of default=True for \n",
    "# a particular logit value (within some small threshold T=1).\n",
    "\n",
    "def get_empirical_likelihood(logit, logits_train, y):  \n",
    "    # selecting all logits in training set that are +/- 0.5 of given logit\n",
    "    valid = ((logits_train < logit + 0.5) & (logits_train > logit - 0.5));    \n",
    "    n_positives = sum(valid[y==True]);\n",
    "    n_negatives = sum(valid[y==False]);\n",
    "    empirical_likelihood = n_positives / (n_positives  + n_negatives);\n",
    "    return empirical_likelihood\n",
    "\n",
    "\n",
    "# taking an ok value for w:\n",
    "w = np.array([-10.63971053,   5.49188453])  \n",
    "\n",
    "# testing set consists of regularly sampled 100 values of balance\n",
    "n_test = 100\n",
    "X_test = np.zeros(shape=(n_test,2))\n",
    "X_test[0:n_test,0] = 1;\n",
    "X_test[0:n_test,1] = np.linspace(X[:,1].min(), X[:,1].max(), num=n_test)\n",
    "\n",
    "# making predictions for these values\n",
    "p_test = lab_2.predict(w, X_test)\n",
    "\n",
    "# we compute the logit values and their corresponding empirical probabilities of default\n",
    "logits_test = lab_2.logit(w, X_test)\n",
    "logits_train = lab_2.logit(w, X)\n",
    "p_empirical = [get_empirical_likelihood(logit, logits_train, y) \n",
    "                  for logit in logits_test ];\n",
    "\n",
    "# plot the graphs\n",
    "\n",
    "plt.scatter(X[y==False,1], y[y==False], alpha= 0.2)\n",
    "plt.scatter(X[y==True,1], y[y==True], alpha= 0.2)\n",
    "plt.plot(X_test[:,1], p_test, color='black')\n",
    "plt.plot(X_test[:,1], p_empirical, ':', color='gray')\n",
    "\n",
    "plt.ylabel('Probability of default');\n",
    "plt.xlabel('Balance');\n",
    "plt.yticks([0, 0.25, 0.5, 0.75, 1.]);\n",
    "plt.legend(['logistic model for probability of default',\n",
    "            'empirically measured probability of default\\n(within a +/- 0.5 logit bin)', \n",
    "            'No Default', 'Default'],  prop={'size': 12})\n",
    "plt.show()\n",
    "\n",
    "# you should be able to see that the logistic model is not a bad approximation \n",
    "# of the empirical likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1664297601534,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "ddMVi3QrQEuf",
    "outputId": "c4def70d-4593-4dfc-f04b-3c9b906e1316"
   },
   "outputs": [],
   "source": [
    "# At this point, we still don't have a classifier. \n",
    "# All we need is to set a threshold on the predicted probabilities\n",
    "\n",
    "# #### EXERCISE 4 ####\n",
    "# In lab_2.py, write a function 'predict_class' in the module to give\n",
    "# the predicted class for observations X and weights w.\n",
    "\n",
    "# Use that function here to assess the accuracy of the classifier\n",
    "# for different thresholds.\n",
    "\n",
    "# Accuracy = percentage correctly classified.\n",
    "def accuracy(w, X, y, threshold):\n",
    "    return np.mean(y == lab_2.predict_class(w, X, threshold))\n",
    "\n",
    "# taking an ok value for w:\n",
    "w = np.array([-10.63971053,   5.49188453])  \n",
    "\n",
    "print(\"Accuracy for T=0.25: {}\".format(accuracy(w, X, y, threshold=0.25)))\n",
    "print(\"Accuracy for T=0.50: {}\".format(accuracy(w, X, y, threshold=0.5)))\n",
    "print(\"Accuracy for T=0.75: {}\".format(accuracy(w, X, y, threshold=0.75)))\n",
    "print(\"Accuracy for T=0.95: {}\".format(accuracy(w, X, y, threshold=0.95)))\n",
    "\n",
    "# #### EXERCISE 5 ####\n",
    "# in lab_2.py, update function 'question_5' to \n",
    "# return the accuracy for a threshold of 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh9SdSMCFrWb"
   },
   "source": [
    "# Multinomial Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1664297619011,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "KgbWuPd7QEuf",
    "outputId": "a8a93149-4e47-4a7f-e444-28d993dfe321"
   },
   "outputs": [],
   "source": [
    "# Let's now see how to work with more than 2 classes.\n",
    "# loading the Iris dataset.\n",
    "# This is a foumous 3-classes dataset, from Fisher, 1936\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris['DESCR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "executionInfo": {
     "elapsed": 2344,
     "status": "ok",
     "timestamp": 1664297629656,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "bOrpJ41JGr0p",
    "outputId": "eab32172-709d-4537-c5e4-7a9b2fd51b82"
   },
   "outputs": [],
   "source": [
    "# let's explore the dataset\n",
    "# as we have 4 features, we will break down the visualisation into pairs of \n",
    "# features\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "class_names = iris.target_names\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "\n",
    "grr = pd.plotting.scatter_matrix(df, c=Y, figsize=(12, 12),grid=True, \n",
    "                                 hist_kwds={'bins': 20}, s=60, alpha=.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1664297651822,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "uHvjhMFINueT"
   },
   "outputs": [],
   "source": [
    "# we are going to split the dataset into training and test tests in a 70/30 ratio\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.3, random_state = 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1664297656267,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "y-tHN_dvN2Ts"
   },
   "outputs": [],
   "source": [
    "# this time we'll use off-the-shelf functions to get the optimisation\n",
    "\n",
    "log_reg = LogisticRegression(solver='newton-cg', multi_class='multinomial',random_state = 11)\n",
    "log_reg.fit(trainX, trainY)\n",
    "y_pred = log_reg.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1664297662340,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "OgfAgsFcN8O7",
    "outputId": "d02f19ff-1908-4581-cf35-3880e27cad9c"
   },
   "outputs": [],
   "source": [
    "print('Accuracy:   {:.2f}'.format(np.mean(testY ==  y_pred)))\n",
    "print('Error rate: {:.2f}'.format(1 - np.mean(testY ==  y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjApazRVOGmT"
   },
   "outputs": [],
   "source": [
    "# #### EXERCISE 6 ####\n",
    "#\n",
    "# Quiz-style: return the bias in weight vector associated with logit of class=0\n",
    "#\n",
    "# hint: biases can be obtained from log_reg.intercept_ \n",
    "#       and the other coefficients from log_reg.coef_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 1121,
     "status": "ok",
     "timestamp": 1664297698225,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "9Xcn9pSAQWxO",
    "outputId": "6f36cb8e-cf0f-4133-aa18-b50777c2a1de"
   },
   "outputs": [],
   "source": [
    "# In multinomial logistic regression, we have one score/logit per class\n",
    "# the logit is obtained from its own weight vector.\n",
    "# We propose here to visualise the score/logit map for each of the classes.\n",
    "\n",
    "# the point of this cell is for you to visualise what the score maps do \n",
    "# just try different class ids and on different feature pairs.\n",
    "\n",
    "classid = 2; # you can change this to 0,1,2\n",
    "\n",
    "# Plot the risk score map (x'w_k) for the associated class weights vector w_k. \n",
    "featx = 1 # feat 0 will be on x-axis, you can change this to 0,1,2,3\n",
    "featy = 2 # feat 1 will be on y-axis, you can change this to 0,1,2,3\n",
    "\n",
    "# getting area of interest\n",
    "\n",
    "x_min = trainX[:,featx].min()\n",
    "x_max = trainX[:,featx].max()\n",
    "y_min = trainX[:,featy].min()\n",
    "y_max = trainX[:,featy].max()\n",
    "\n",
    "# grid sampling the feature space\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 256), np.linspace(y_min, y_max, 256))\n",
    "\n",
    "# making prediction for point on grid\n",
    "Z = log_reg.coef_[classid,featx] * xx + yy * log_reg.coef_[classid,featy] \n",
    "\n",
    "# plot score map\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.contourf(xx, yy, Z, levels=10, alpha=0.9)\n",
    "# overlay dataset points\n",
    "scatter = plt.scatter(trainX[:, featx], trainX[:, featy], c=(trainY==classid), alpha=0.8)\n",
    "\n",
    "kw = scatter.legend_elements()\n",
    "kw[1][0] = f\"other classes\"\n",
    "kw[1][1] = f\"class {classid} ({class_names[classid]})\"\n",
    "plt.legend(*kw)\n",
    "\n",
    "plt.xlabel(f\"Feature {featx} - {feature_names[featx]}\")\n",
    "plt.ylabel(f\"Feature {featy} - {feature_names[featy]}\")\n",
    "plt.title(f\"risk score map for class {classid} ({class_names[classid]})\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1664297805317,
     "user": {
      "displayName": "Francois Pitie",
      "userId": "11315581241956008928"
     },
     "user_tz": -60
    },
    "id": "QOTB1yiC5E6d",
    "outputId": "5fc92294-9497-4a6d-df48-6be12f2cd0f7"
   },
   "outputs": [],
   "source": [
    "# #### EXERCISE 7 ####\n",
    "# \n",
    "# In this exercise, you must find the best pair of features in the iris dataset.\n",
    "#\n",
    "# To find this pair, you will consider every possible pair and reduce the \n",
    "# input features to only that pair (and ignore the other two features). \n",
    "#\n",
    "# hint:\n",
    "# iterate through all possible pairs and for each pair:\n",
    "# 1. modify both training and test sets to only include the two considered features\n",
    "# 2. train a multinomial logistic regression model based on this reduced feature set\n",
    "# 3. make prediction on the reduced test set \n",
    "# 4. report accuracy for that pair\n",
    "#\n",
    "# you can work in this notebook, but you need to report the best pair \n",
    "# and the accuracy in question_7 in lab_2.py\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}